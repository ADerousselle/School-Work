{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d657d3d8-5907-48ed-bb21-59a5f40cc35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTS\n",
    "import requests\n",
    "import json\n",
    "import sys\n",
    "import time\n",
    "import os\n",
    "import smtplib\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.text import MIMEText\n",
    "from dotenv import dotenv_values\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "#ENV_PARSER DEFINITION\n",
    "def env_parser(env_values):\n",
    "    \"\"\"\n",
    "    Parses the sensitive data of an .env file into 3 variables needed to run the script.\n",
    "\n",
    "    Parameters:\n",
    "    env_values (dictionary): A dictionary containing all of the sensititve data needed to run the script.\n",
    "\n",
    "    Returns:\n",
    "    ph_config (dictionary): Contains the API key and search term field needed to run a parsehub project.\n",
    "    em_config (dictionary): Contains the email address of the sender and receiver and the sender's password.\n",
    "    projects (list): Tuples each containing a project token, store name, a string representing a space, a url template\n",
    "    \"\"\"\n",
    "    \n",
    "    env_values = dict(env_values)                               #Read env_values into a dictionary of the same name\n",
    "    projects = json.loads(env_values[\"projects\"])               #A list of tuples each containing:\n",
    "                                                                    #A project token to a project that scrapes a particular store\n",
    "                                                                    #The related store's name\n",
    "                                                                    #The string used in that store's URLs to represent a space in a search\n",
    "                                                                    #A URL template that when contatenated with a search \"term\" will pull up a search results page\n",
    "    ph_config = {                                               #The configurations needed to run any parsehub project\n",
    "        \"api_key\":env_values[\"api_key\"],                        #The developer's API key\n",
    "        \"start_url\":env_values[\"start_url\"]                     #The URL to start the scraping from, Default: None\n",
    "    }\n",
    "    em_config = {                                               #The configurations needed to send an error email to the developer\n",
    "        \"sender_email\":env_values[\"sender_email\"],              #The email address of the message sender\n",
    "        \"app_password\":env_values[\"app_password\"],              #The password to the email address of the message sender\n",
    "        \"receiver_email\":env_values[\"receiver_email\"]           #The email address of the message receiver\n",
    "    }\n",
    "\n",
    "    return ph_config, em_config, projects                       #Return the Parsehub configurations, email configurations, and project tuples\n",
    "\n",
    "#URL_CREATOR DEFINITION\n",
    "def url_creator(email_config, searchTerms, project):\n",
    "    \"\"\"\n",
    "    Concatenates a URL template for a specific website with a particular search term to create a URL. If the URL is valid it is added to a list of URLs if not an error\n",
    "    message is sent.\n",
    "\n",
    "    Parameters:\n",
    "    email_config (dictionary): contains the key:value pairs needed to send an email.\n",
    "    searchTerms (list): A list of strings representing categories and subcategories of products that need to be scraped from a website.\n",
    "    project (tuple): A tuple of strings that contains the string representing a space, and a URL template needed to create a real URL.\n",
    "\n",
    "    Returns:\n",
    "    url_list (list): Contains all the valid URLs created.\n",
    "    \"\"\"\n",
    "    \n",
    "    #URL Specific Variable Declaration\n",
    "    space = project[2]                                          #The string representing a space in the particular store's URLs\n",
    "    template = project[3]                                       #The URL template that when concatented with a search \"term\" produces a search results page\n",
    "    url_list = []                                               #An empty list to hold the newly created URLs\n",
    "    \n",
    "    #Iterate through the terms in the searchTerms.txt file, using them to create URLs\n",
    "    with open(searchTerms, 'r') as file:                        #Open and read from the searchTerms.txt file\n",
    "        for term in file:                                           #For each line in the file    \n",
    "            #Read and alter the lines to fit the store's URL schema and then form a complete URL\n",
    "            name = term.strip()                                         #Read the line as a name with no trailing spaces\n",
    "            name = name.replace(\" \", space)                             #Replace any spaces between words with the string used to replace spaces in the store's URLs\n",
    "            url = template + name                                       #Concatenate the template and the name to form a complete URL\n",
    "            \n",
    "            #Call is_valid_url() on url, respond appropriately\n",
    "            if is_valid_url(url):                                       #If url is valid\n",
    "                url_list.append(url)                                        #Append it to the url list\n",
    "            else:                                                       #If url is not valid\n",
    "                send_error(1, url, email_config)                           #Call send_error with a code of 1 and the url\n",
    "                \n",
    "    return url_list                                             #Return the completed URL list\n",
    "\n",
    "#IS_VALID_URL DEFINITION\n",
    "def is_valid_url(url):\n",
    "    \"\"\"\n",
    "    Uses the request module to determine if a URL leads to a webpage that exists.\n",
    "\n",
    "    Parameters:\n",
    "    url (string): Represents a url that should lead to a webpage that exists\n",
    "\n",
    "    Returns:\n",
    "    bool: True if a URL leads to a page that exists, false if for any reason the request was unable to reach an existing webpage.\n",
    "    \"\"\"\n",
    "    \n",
    "    #Send a request for the url and get it's status code if possible, return true or false based on results\n",
    "    try:\n",
    "        response = requests.head(url)                           #Send a request for the metadata of the URL\n",
    "        return response.status_code != 404                    #Return true if the URL exists\n",
    "    except requests.exceptions.RequestException:                #If for any reason the URL was not able be pulled up\n",
    "        return False                                                #Return false\n",
    "\n",
    "#RUN_PROJ DEFINITION\n",
    "def run_proj(url, err_count, ph_config, em_config, proj_token):\n",
    "    \"\"\"\n",
    "    Runs a particular parsehub project on a particular URL, waits for the run to complete, checks that the run was successful and if not retry the run (a maximum of two\n",
    "    times). Pulls the successfully scraped data and converts it to a dictionary.\n",
    "\n",
    "    Parameters:\n",
    "    url (string): A valid URL leading to a search results webpage for a particular category or subcategory.\n",
    "    err_count (int): A number indicating the number of times a run was done on a particular URL for a particular project.\n",
    "    ph_config (dictionary): A dictionary containing key:value pairs needed to run a parsehub project.\n",
    "    em_config (dictionary): A dictionary containing key:value pairs needed to send an email.\n",
    "    proj_token (string): A string representing a particular parsehub project in the parsehub server.\n",
    "\n",
    "    Returns:\n",
    "    data_dict (dictionary): A dictionary containing all of the product data scraped from a particular URL using a particular project.\n",
    "    \"\"\"\n",
    "    \n",
    "    #Setup and start a project run\n",
    "    ph_config[\"start_url\"]=url                                  #Set the \"start_url\" to the current URL\n",
    "                                                                #Start a project run using the ph_config and proj_token, return the data associated with the project run\n",
    "    r = requests.post(f'https://www.parsehub.com/api/v2/projects/{proj_token}/run', data=ph_config)\n",
    "    rjson = r.json()                                            #Parse project run data into a dictionary\n",
    "    run_token = rjson['run_token']                              #Extract the run_token from the dictionary\n",
    "                                                                                                            \n",
    "    #Continue to check the status of the run until it either completes successfully or not, handle accordingly\n",
    "    while True:\n",
    "        run = requests.get(f'https://www.parsehub.com/api/v2/runs/{run_token}', params=ph_config)   #Pull the run data at the moment using the ph_config and run_token\n",
    "        run_json = run.json()                                                                       #Parse the run data into a dictionary\n",
    "        run_status = run_json['status']                                                             #Extract the status of the run from the dictionary\n",
    "        print(run_status)\n",
    "        if (run_status == 'complete' or run_status == 'error'):                                     #If the status is 'complete' or 'error'\n",
    "            if (run_status == 'error'):                                                                 #If the status is 'error'\n",
    "                if err_count == 1:                                                                          #If the error count is 1\n",
    "                    send_error(2, url, em_config)                                                              #Call send_error with a code of 2 and the url\n",
    "                    data_dict = {}                                                                              #Create an empty dictionary to return\n",
    "                else:                                                                                       #If the error count is less than 1\n",
    "                    data_dict = run_project(url, 1, ph_congfig, em_config)                                      #Try running the project again with an error count of 1 instead\n",
    "            elif(run_status == 'complete'):                                                             #If the status is 'complete'\n",
    "                data_json =requests.get(f'https://www.parsehub.com/api/v2/runs/{run_token}/data', params=ph_config)#Get the scraped data from the run using ph_config and run_token \n",
    "                if data_json == {}:\n",
    "                    return {}\n",
    "                else:\n",
    "                    data_dict = data_json.json()                                                                   #Convert the returned JSON to a dictionary\n",
    "            break                                                                                       #Exit the loop\n",
    "        time.sleep(60)                                                                              #Else wait for 60 seconds before checking the status again\n",
    "    return data_dict                                            #Return the scraped data in dictionary format\n",
    "\n",
    "#DEBUG_SEND_ERROR DEFINITION\n",
    "def debug_send_error(code, string):                         #DEBUG: Print a message to the screen instead of emailing\n",
    "    \"\"\"\n",
    "    This function is for debugging purposes only, and is meant to replace send_error() during debugging. It reduces the number of emails sent to the developer, \n",
    "    by instead printing the messages to the screen.\n",
    "\n",
    "    Parameters:\n",
    "    code (int): An error code, indicating which error message to use.\n",
    "    string (string): A word or url that clarifies what exactly is causing the error.\n",
    "\n",
    "    Returns:\n",
    "    Nothing\n",
    "    \"\"\"\n",
    "    \n",
    "    if code == 1:                                               #If the code is 1\n",
    "        print(f\"ERROR 1: INVALID URL\\n{string}\")                    #Print that the URL is invalid\n",
    "    elif code == 2:                                             #If the code is 2\n",
    "        print(f\"ERROR 2: UNABLE TO SCRAPE URL\\n{string}\")           #Print that the project is unable to scrape data from the URL\n",
    "    elif code == 3:                                             #If the code is 3\n",
    "        print(f\"ERROR 3: UNABLE TO SCRAPE FIELD\\n{string}\")         #Print that the project is not scraping the field\n",
    "\n",
    "#SEND_ERROR DEFINITION\n",
    "def send_error(code, string, config):\n",
    "    \"\"\"\n",
    "    This function is meant to be used when not debugging. It sends a personalized message to the developer about an error or mistake made in the script or \n",
    "    the parsehub project.\n",
    "\n",
    "    Parameters:\n",
    "    code (int): An error code, indicating which error message to use.\n",
    "    string (string): A word or url that clarifies what exactly is causing the error.\n",
    "    config (dictionary): A dictionary of key:value pairs needed to send send an email.\n",
    "\n",
    "    Returns:\n",
    "    Nothing\n",
    "    \"\"\"\n",
    "    \n",
    "    #Get sender and receiver data from config\n",
    "    sender_email = config[\"sender_email\"]                       #Set the sender email address\n",
    "    app_password = config[\"app_password\"]                       #Set the sender email password\n",
    "    receiver_email = config[\"receiver_email\"]                   #Set the receiver email address\n",
    "    \n",
    "    # Create an email message\n",
    "    msg = MIMEMultipart()                                       #Create a message\n",
    "    msg['From'] = sender_email                                  #Set the sender to the sender email address\n",
    "    msg['To'] = receiver_email                                  #Set the receiver to the receiver email address\n",
    "    msg['Subject'] = \"Error Running Parsehub Script\"            #Set the Subject of the email\n",
    "\n",
    "    #Set and attach the body to the message\n",
    "    if code == 1:                                               #If the code is 1 set the body of the message to show that the URL is invalid\n",
    "        body = f\"BobcatCLAWS Dev Team,\\nThe Parsehub script was unable to validate the url:\\n{string}\\nPlease inspect the url and make any needed corrections in the script.\"\n",
    "    elif code == 2:                                             #If the code is 2 set the body of the message to show that the project cannot scrape data from a URL\n",
    "        body = f\"BobcatCLAWS Dev Team,\\nThe Parsehub script was unable to scrape the url:\\n{string}\\nPlease inspect the url and project and make any needed corrections.\"\n",
    "    elif code == 3:                                             #If the code is 3 set the body of the message to show that the project cannot scrape some field\n",
    "        body = f\"BobcatCLAWS Dev Team,\\nThe Parsehub script was unable to scrape {string} data.\\nPlease inspect the Parsehub project and make any needed corrections.\"\n",
    "    msg.attach(MIMEText(body, 'plain'))                         #Attach the body to the message\n",
    "\n",
    "    #Connect to the Gmail's SMTP server\n",
    "    smtp_server = 'smtp.gmail.com'\n",
    "    smtp_port = 587\n",
    "    smtp_connection = smtplib.SMTP(smtp_server, smtp_port)\n",
    "\n",
    "    # Start the TLS (Transport Layer Security) session\n",
    "    smtp_connection.starttls()\n",
    "    \n",
    "    # Log in to the email account and send the message\n",
    "    smtp_connection.login(sender_email, app_password)\n",
    "    smtp_connection.sendmail(sender_email, receiver_email, msg.as_string())\n",
    "    \n",
    "    # Close the SMTP connection\n",
    "    smtp_connection.quit()\n",
    "\n",
    "#CHECK_VALUES DEFINITION\n",
    "def check_values(dict, config):\n",
    "    \"\"\"\n",
    "    Checks that the parsehub project was able to successfully scrape particular product fields. If it was determined that a field is not being properly scraped the field is \n",
    "    added to the dictionary with the best possible value and an error message is sent to the developer.\n",
    "\n",
    "    Parameters:\n",
    "    dict (dictionary): A dictionary containing a \"Product\" list consisting of product dictionaries with various fields which may or may not have values.\n",
    "\n",
    "    Returns:\n",
    "    dict (dictionary): A dictionary containing a \"Product\" list consisting of product dictionaries \n",
    "    \"\"\"\n",
    "    \n",
    "    #Flag Variabel Declarations\n",
    "    name_flag = 1                                               #Flag indicating that the project is not scraping product names\n",
    "    price_flag = 1                                              #Flag indicating that the project is not scraping product prices\n",
    "    img_flag = 1                                                #Flag indicating that the project is not scraping product image URLs\n",
    "\n",
    "    #Iterate through dictionary to evaluate the presence and values of fields in each product\n",
    "    for prod in dict[\"Product\"]:                                #For each product in the dictionary\n",
    "\n",
    "        #Handle missing \"Name\" and \"Description\" fields\n",
    "        if \"Name\" not in prod:                                       #If the Name is not in the product dictionary\n",
    "            prod[\"Name\"] = \"\"                                           #Add it to the dictionary with an empty string as the value\n",
    "        if \"Description\" not in prod:                                #If the Description is not in the product dictionary\n",
    "            prod[\"Description\"] = \"\"                                    #Add it to the dictionary with an empty string as the value\n",
    "        if prod[\"Name\"] != \"\" and prod[\"Description\"] != \"\":         #If the product Name and Description are filled\n",
    "            name_flag = 0                                               #Change the name flag to indicate that the project is scraping product names\n",
    "        elif prod[\"Name\"] == \"\" and prod[\"Description\"] != \"\":       #If the product Name is not filled but the Description is filled\n",
    "            prod[\"Name\"] = prod.get(\"Description\")                      #Fill the Name with the Description's value\n",
    "        elif prod[\"Description\"] == \"\" and prod[\"Name\"] != \"\":       #If the product Description is not filled but the Name is filled\n",
    "            prod[\"Description\"] = prod.get(\"Name\")                      #Fill the Description with the Name's value\n",
    "\n",
    "        #Handle missing \"Price\" field\n",
    "        if \"Price\" not in prod:                                     #If the Price is not in the product dictionary (Not all products have prices on the web page)\n",
    "            prod[\"Price\"] = \"Check store for price\"                     #Add it to the dictionary with a value indicating to check the store page for the price\n",
    "        if prod[\"Price\"] != \"Check store for price\":                #If the product Price is filled\n",
    "            price_flag = 0                                              #Change the price flag to indicate that the project is scraping product prices\n",
    "\n",
    "        #Handle missing \"Img_URL\" field\n",
    "        if \"Img_URL\" not in prod:                                   #If the Img_URL is not in the product dictionary\n",
    "            prod[\"Img_URL\"] = \"No Image\"                                #Set the Img_URL to the string No Image\n",
    "        elif prod[\"Img_URL\"] != \"No Image\":                         #If the product Img_URL is filled\n",
    "            img_flag = 0                                                #Change the img flag to indicate that the project is scraping image URLs\n",
    "    \n",
    "    #Send error messages for all flags indicating that a field is not being scraped by the project\n",
    "    if name_flag == 1:                                          #If the name flag still indicates that names are not being scraped\n",
    "        send_error(3, \"Name and Description\", config)                #Call send_error with a code of 3 and the field \"Name\"\n",
    "    if price_flag == 1:                                         #If the price flag still indicates that prices are not being scraped\n",
    "        send_error(3, \"Price\", config)                             #Call send_error with a code of 3 and the field \"Price\"\n",
    "    if img_flag == 1:                                           #If the img flag still indicates that image URLs are not being scraped\n",
    "        send_error(3, \"Img_URL\", config)                           #Call send_error with a code fo 3 and the field \"Img_URL\"\n",
    "    return dict                                                 #Return the newly checked and filled dictionary\n",
    "\n",
    "#FORMATTER DEFINITION\n",
    "def formatter(dict):\n",
    "    \"\"\"\n",
    "    Takes a dictionary's \"Product\" list, which consists of dictionaries of product data, and reorganizes the fields in each product dictionary in a desired format.\n",
    "\n",
    "    Parameters:\n",
    "    dict (dictionary): A dictionary containing a list called \"Products\" that consists of product dictionaries with unorganized fields.\n",
    "\n",
    "    Returns:\n",
    "    formated_list (list): A list of product dictionaries with organized fields.\n",
    "    \"\"\"\n",
    "    #Create and fill a new list with the dictionaries of product data in the desired order\n",
    "    formated_list = []                                          #Create an empty list to fill with formatted product dictionaries\n",
    "    for raw_prod in dict[\"Product\"]:                            #For each raw product dictionary in the dictionary's Product list\n",
    "        prod = {                                                    #Create a new product dictionary with the fields in the following order:\n",
    "            \"URL\": raw_prod.get(\"URL\"),                                 #Product page URL\n",
    "            \"UPC\": \"placeholder_value\",                                 #Default product UPC field\n",
    "            \"Name\": raw_prod.get(\"Name\"),                               #Product name\n",
    "            \"Price\": raw_prod.get(\"Price\"),                             #Product price\n",
    "            \"Category_ID\": raw_prod.get(\"Category_ID\"),                 #Product category\n",
    "            \"Sub_Category_ID\": raw_prod.get(\"Sub_Category_ID\"),         #Product subcategory\n",
    "            \"Description\": raw_prod.get(\"Description\"),                 #Product description (same value as name)\n",
    "            \"Keywords\": [],                                             #Empty list of product keywords\n",
    "            \"Img_URL\": raw_prod.get(\"Img_URL\")                          #Product image URL\n",
    "        }\n",
    "        formated_list.append(prod)                                  #Attach the new product dictionary to the end of the new list\n",
    "    return formated_list                                        #Return the new list of formatted product dictionaries\n",
    "\n",
    "#RM_DUPLICATE DEFINITION\n",
    "def rm_duplicate(list):\n",
    "    \"\"\"\n",
    "    Removes any duplicate product dictionaries in the given list by comapring the name fields of each dictionary.\n",
    "\n",
    "    Parameters:\n",
    "    list (list): A list of dictionaries each containing the data for a single product, some dictionaries may be duplicates.\n",
    "\n",
    "    Returns:\n",
    "    filtered_list (list): A list of dictionaries each containing the data for a single unique product.\n",
    "    \"\"\"\n",
    "    \n",
    "    #Duplicate Removal Specific Variable Declaration\n",
    "    seen_names = set()                                          #Create a set to keep track of seen names\n",
    "    filtered_list = []                                          #Create a new list to store the filtered dictionaries\n",
    "\n",
    "    #Iterate through the products of the list looking for and removing duplicate products\n",
    "    for prod in list:                                           #For each product in the list\n",
    "        name = prod[\"Name\"]                                         #Retrieve the product name\n",
    "        if name not in seen_names:                                  #If the name has not already been seen\n",
    "            seen_names.add(name)                                        #Add the name to the set of seen names\n",
    "            filtered_list.append(prod)                                  #Add the dictionary to the filtered list\n",
    "    return filtered_list                                        #Return the new list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70c00189-34c7-4893-8a2a-90e445160365",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'ph.env'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 16\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#MAIN DEFINITION\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#def main():\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#    \"\"\"\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m#Script Specific Variable Declaration\u001b[39;00m\n\u001b[0;32m     15\u001b[0m env_values \u001b[38;5;241m=\u001b[39m dotenv_values(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mph.env\u001b[39m\u001b[38;5;124m\"\u001b[39m)                            \u001b[38;5;66;03m#Create a dictionary using the data in a .env file\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mph.env\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContents of .env file:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(f\u001b[38;5;241m.\u001b[39mread())\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\SEP\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    284\u001b[0m     )\n\u001b[1;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ph.env'"
     ]
    }
   ],
   "source": [
    "#MAIN DEFINITION\n",
    "#def main():\n",
    "#    \"\"\"\n",
    "#    Main function that creates all the shared variables, then runs a loop over each project and each url for the store the project is associated with so as to scrape all \n",
    "#    the necesary product data from a particular website using a particular pareshub project.\n",
    "#    \n",
    "#    Paramerters:\n",
    "#    None\n",
    "#    \n",
    "#    Returns:\n",
    "#    None\n",
    "#    \"\"\"\n",
    "\n",
    "#Script Specific Variable Declaration\n",
    "env_values = dotenv_values(\"ph.env\")                            #Create a dictionary using the data in a .env file\n",
    "ph_config, em_config, projects = env_parser(env_values)         #Call env_parser(), return parsed data in three variables\n",
    "\n",
    "json_path = \"../json_files/\"                                    #Path tot he location all of the JSON output files will be placed\n",
    "searchTerms = \"searchTerms.txt\"                                 #Path to the text file used to generate urls\n",
    "\n",
    "#Iterate through the project tuples, performing necessary operations to create a final JSON containing all data scraped from the relevant store.\n",
    "for project in projects:                                        #For each project\n",
    "    #Project Specific Variable Declaration\n",
    "    final_dict = {                                              #Initialize dictionary that will be saved as a JSON for a particular store\n",
    "        \"Store\": project[1],                                    #Holds store name\n",
    "        \"Products\": []                                          #List of dictionaries of product data\n",
    "    }\n",
    "    final_list = []                                             #List that will be used to perform operation on the data before being passed to \"Products\" in final_dict\n",
    "    url_list = url_creator(em_config, searchTerms, project)     #Call url_creator(), return a list of URLs to scrape\n",
    "    \n",
    "    #Iterate through the url_list, scraping, checking, and formating the product data dictionaries\n",
    "    for url in url_list:                                                #For each url\n",
    "        raw_dict = run_proj(url, 0, ph_config, em_config, project[0])   #Call run_proj on the current url with an error count of 0, return a dictionary of scraped data\n",
    "        if raw_dict == {}:\n",
    "            continue\n",
    "        checked_dict = check_values(raw_dict, em_config)                #Call check_values, return the corrected dictionary\n",
    "        formatted_list = formatter(checked_dict)                        #Call formatter, return a list with the fields in the dictionaries in the desired order.\n",
    "        final_list.extend(formatted_list)                               #Combine the formated list with the growing final_list list\n",
    "                                                                        \n",
    "    \n",
    "    #Remove all duplicate product dictionaries in the final list and insert the list into final_dict[\"Products\"]\n",
    "    filtered_list = rm_duplicate(final_list)                    #Call rm_duplicate, return a list with no duplicate product dictionaries\n",
    "    final_dict[\"Products\"].extend(filtered_list)                #Set \"Products\" in final_dict to the filtered_list\n",
    "\n",
    "    #Save the final_dict as a JSON file with the store name as the file name.\n",
    "    path = json_path + project[1] + \".json\"                     #Define the full path to the JSON, as well as name the JSON after the store the data comes from \n",
    "    with open(path, 'w') as json_file:                          #Create and write to the file defined in path\n",
    "        json.dump(final_dict, json_file, indent = 4)            #Convert final_dict to a JSON, displayed vertically, to be written to the JSON file\n",
    "    print(\"Scraping Complete, JSON created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304b8b95-db27-4743-a893-6a6091672559",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
