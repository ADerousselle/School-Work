{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cb27a76-beb1-40c0-98c3-3b8490ea68f4",
   "metadata": {},
   "source": [
    "Task 1 [50 pts]: Implement an algorithmically efficient version of object detection using the chamfer distance. In particular, implement a function \r\n",
    "\r\n",
    "(scores, result_image) = chamfer_search(edge_image, template, scale, number_of_results),\r\n",
    "\r\n",
    "where:\r\n",
    "\r\n",
    "edge_image is the image (2D matrix) you want to search.\r\n",
    "template is the pattern (2D matrix) you are searching for.\r\n",
    "scale A scale s means that the template size must be multiplied by s in order to match the occurrence of the object in the image.\r\n",
    "number_of_results specifies the number of results that will be displayed on result_image (note: result_image is an output argument).\r\n",
    "scores is a matrix of size equal to the size of the image, and scores(i,j) is the directed chamfer distance from the template to a window centered at (i, j).\r\n",
    "result_image is a copy of edge_image, with white bounding rectangles drawn in white color for the best matches found during the search. The number of bounding rectangles to be drawn is specified by the input argument number_of_results. The centers of these bounding rectangles are simply the pixel locations in scores (which is the first output argument). Note: you may use the given draw_rectangle function to draw the bounding rectangles.\r\n",
    "Tip: To avoid many bounding rectangles appearing on top of each other, do not allow more than 50% overlap between two rectangles, i.e., the center of one rectangle should not be allowed within the region defined by a previous rectangle. Note: this should not change the scores matrix that is returned by the function, so make sure to make a copy of it inside your function.\r\n",
    "Your function will be graded based on the correctness and efficiency of the implementation. As usual, feel free to use any functions that OpenCV already defines (such as cv.distanceTransform()), or code posted on the course repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c7b427b-93d9-4312-8f3f-9a676569c83d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05707732\n",
      "171\n",
      "222\n",
      "0.05707732\n",
      "171\n",
      "222\n",
      "0.099219725\n",
      "112\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "#VERSION1\n",
    "import cv2\n",
    "import numpy as np\n",
    "from draw_rectangle import draw_rectangle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "edge_image = cv2.imread('data/clutter1_edges.bmp', cv2.IMREAD_GRAYSCALE)\n",
    "template = cv2.imread('data/template.bmp', cv2.IMREAD_GRAYSCALE)\n",
    "scale = 1\n",
    "number_of_results = 5\n",
    "\n",
    "# Apply distance transform to the edge image\n",
    "edge_image_inv = cv2.bitwise_not(edge_image)\n",
    "distance_transform = cv2.distanceTransform(edge_image_inv, cv2.DIST_L2, 5)\n",
    "\n",
    "# Resize the template and convert to uint8\n",
    "resized_template = cv2.resize(template, None, fx=scale, fy=scale).astype(np.uint8)\n",
    "\n",
    "# Compute Chamfer distance scores\n",
    "convolution_scores = cv2.filter2D(distance_transform, -1, resized_template)\n",
    "\n",
    "# Normalize the result\n",
    "scores = convolution_scores / convolution_scores.max()\n",
    "scores_copy = scores.copy()\n",
    "\n",
    "\n",
    "min_score = np.min(scores)\n",
    "row, col = np.unravel_index(np.argmin(scores, axis=None), scores.shape)\n",
    "print(min_score)\n",
    "print(row)\n",
    "print(col)\n",
    "\n",
    "#print(\"Scores Copy:\")\n",
    "#for row in scores_copy:\n",
    "#    for value in row:\n",
    "#        print(f\"{value:.4f}\", end=' ')\n",
    "#    print()\n",
    "\n",
    "# Find top matches without overlap\n",
    "top_matches = []\n",
    "for _ in range(number_of_results):\n",
    "    # Find the minimum location in the current result\n",
    "    min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(scores_copy)\n",
    "    top_matches.append(min_loc)\n",
    "\n",
    "    # Set the pixels within the region of the current match to 1\n",
    "    y1 = min_loc[1] - int(resized_template.shape[0] / 2)\n",
    "    y2 = min_loc[1] + int(resized_template.shape[0] / 2)\n",
    "    x1 = min_loc[0] - int(resized_template.shape[1] / 2)\n",
    "    x2 = min_loc[0] + int(resized_template.shape[1] / 2)\n",
    "    scores_copy[y1:y2, x1:x2] = 1\n",
    "\n",
    "#print(\"Scores:\")\n",
    "#for row in scores:\n",
    "#    for value in row:\n",
    "#        print(f\"{value:.4f}\", end=' ')  # Print each value separated by a space\n",
    "#    print()  # Move to the next line for the next row\n",
    "\n",
    "# Create result image and draw bounding rectangles\n",
    "result_image = cv2.imread('data/clutter1_edges.bmp')\n",
    "\n",
    "for (x, y) in top_matches:\n",
    "    top = y + int(resized_template.shape[0] / 2)\n",
    "    bottom = y - int(resized_template.shape[0] / 2)\n",
    "    left = x - int(resized_template.shape[1] / 2)\n",
    "    right = x + int(resized_template.shape[1] / 2)\n",
    "    result_image = draw_rectangle(result_image, top, bottom, left, right)\n",
    "\n",
    "min_score = np.min(scores)\n",
    "row, col = np.unravel_index(np.argmin(scores, axis=None), scores.shape)\n",
    "print(min_score)\n",
    "print(row)\n",
    "print(col)\n",
    "\n",
    "min_score = np.min(scores_copy)\n",
    "row, col = np.unravel_index(np.argmin(scores_copy, axis=None), scores.shape)\n",
    "print(min_score)\n",
    "print(row)\n",
    "print(col)\n",
    "    \n",
    "#plt.imshow(result_image)\n",
    "#print(top_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63180060-dffc-424b-84b3-6f98280f44e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#VERSION2\n",
    "import cv2\n",
    "import numpy as np\n",
    "from draw_rectangle import draw_rectangle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "edge_image = cv2.imread('data/clutter1_edges.bmp', cv2.IMREAD_GRAYSCALE)\n",
    "template = cv2.imread('data/template.bmp', cv2.IMREAD_GRAYSCALE)\n",
    "scale = 1\n",
    "number_of_results = 5\n",
    "\n",
    "# Apply distance transform to the edge image\n",
    "edge_image_inv = cv2.bitwise_not(edge_image)\n",
    "distance_transform = cv2.distanceTransform(edge_image_inv, cv2.DIST_L2, 5)\n",
    "\n",
    "# Resize the template and convert to uint8\n",
    "resized_template = cv2.resize(template, None, fx=scale, fy=scale).astype(np.uint8)\n",
    "\n",
    "# Compute Chamfer distance scores\n",
    "convolution_scores = cv2.filter2D(distance_transform, -1, resized_template)\n",
    "\n",
    "# Normalize the result\n",
    "scores = convolution_scores / convolution_scores.max()\n",
    "\n",
    "\n",
    "min_score = np.min(scores)\n",
    "row, col = np.unravel_index(np.argmin(scores, axis=None), scores.shape)\n",
    "print(min_score)\n",
    "print(row)\n",
    "print(col)\n",
    "\n",
    "\n",
    "# Find top matches without overlap\n",
    "top_matches = []\n",
    "for _ in range(number_of_results):\n",
    "    # Find the minimum location in the current result\n",
    "    min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(scores)\n",
    "    top_matches.append(min_loc)\n",
    "\n",
    "    # Set the pixels within the region of the current match to 1\n",
    "    y1 = min_loc[1] - int(resized_template.shape[0] / 2)\n",
    "    y2 = min_loc[1] + int(resized_template.shape[0] / 2)\n",
    "    x1 = min_loc[0] - int(resized_template.shape[1] / 2)\n",
    "    x2 = min_loc[0] + int(resized_template.shape[1] / 2)\n",
    "    scores[y1:y2, x1:x2] = 1\n",
    "\n",
    "# Create result image and draw bounding rectangles\n",
    "result_image = cv2.imread('data/clutter1_edges.bmp')\n",
    "\n",
    "for (x, y) in top_matches:\n",
    "    top = y + int(resized_template.shape[0] / 2)\n",
    "    bottom = y - int(resized_template.shape[0] / 2)\n",
    "    left = x - int(resized_template.shape[1] / 2)\n",
    "    right = x + int(resized_template.shape[1] / 2)\n",
    "    result_image = draw_rectangle(result_image, top, bottom, left, right)\n",
    "\n",
    "plt.imshow(result_image)\n",
    "print(top_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d12430f-7c77-44c3-b6c4-3a5300071c96",
   "metadata": {},
   "source": [
    "Task 2 [50 pts]: Implement a function called (scores, result_image) = skin_chamfer_search(color_image, edge_image, template, scale, number_of_results) that, in addition to your solution to Task 1, also uses histogram-based skin detection to improve results, for cases where we are interested in detecting hands.\n",
    "Your goal for this task is to improve detection accuracy by combining skin detection and chamfer distance. When combining these two, you should expect the hand to be detected as the top 1 result.\n",
    "The skin histograms are available in the data folder and can be loaded inside your function using np.load(). The function detect_skin() is also provided.\n",
    "The detection speed can also be improved using this technique, however, the way to achieve that is more advanced, and it is not required for this assignment. You can try it optionally if you wish.\n",
    "An example of a test image, corresponding edge image, and template, that can be passed to this function, is seen in Figure 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6cdc0ac6-cc14-4628-a12e-3d4948698454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11750685\n",
      "112\n",
      "91\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from draw_rectangle import draw_rectangle\n",
    "from detect_skin import detect_skin\n",
    "from chamfer_search import chamfer_search\n",
    "\n",
    "color_image = cv2.imread('data/clutter1.bmp')\n",
    "color_image = cv2.cvtColor(color_image, cv2.COLOR_BGR2RGB)\n",
    "edge_image = cv2.imread('data/clutter1_edges.bmp', cv2.IMREAD_GRAYSCALE)\n",
    "template = cv2.imread('data/template.bmp', cv2.IMREAD_GRAYSCALE)\n",
    "scale = 1\n",
    "number_of_results = 1\n",
    "threshold = .05\n",
    "pos_hist = np.load('data/positive_histogram.npy')\n",
    "neg_hist = np.load('data/negative_histogram.npy')\n",
    "\n",
    "skin_mask = detect_skin(color_image, pos_hist, neg_hist)  # Load histograms using np.load()\n",
    "\n",
    "# Step 2: Apply skin mask to the edge image\n",
    "edge_image_skin = edge_image.copy()\n",
    "edge_image_skin[skin_mask < threshold] = 0\n",
    "\n",
    "\n",
    "# Step 3: Use chamfer search in the skin-filtered edge image\n",
    "scores, result_image = chamfer_search(edge_image_skin, template, scale, number_of_results)\n",
    "\n",
    "min_score = np.min(scores)\n",
    "row, col = np.unravel_index(np.argmin(scores, axis=None), scores.shape)\n",
    "print(min_score)\n",
    "print(row)\n",
    "print(col)\n",
    "\n",
    "# Step 4: Return the top match's score and resulting image\n",
    "top_score = scores[0]\n",
    "top_result_image = result_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe9dada-0386-45d3-ae69-d98a1c9d0f7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
