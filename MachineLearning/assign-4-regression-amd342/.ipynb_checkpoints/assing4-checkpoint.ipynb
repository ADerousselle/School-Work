{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS4347 Assignment 4: Regression on the Wine Quality Dataset\n",
    "\n",
    "## Dataset\n",
    "The Wine Quality dataset consists of 11 input variables and 1 output variable, which is the quality of the wine on a scale of 0 to 10. The input variables include information about the physicochemical properties of the wine, such as its acidity, pH, and alcohol content. The dataset contains two files, `winequality-white.csv` with 4898 samples and `winequality-red.csv` with 1599 samples. The dataset is available at https://archive.ics.uci.edu/ml/datasets/wine+quality. In this assignment we will use the white wine dataset. The dataset has been downloaded for you and is available in the `data` folder.\n",
    "\n",
    "## Task 1 [50 points]: Compare the performance of linear regression, kNN regression, and decision tree regression implemented in the Scikit-learn library on the Wine Quality dataset.\n",
    "\n",
    "In each one of the steps below, you can import the necessary libraries required to complete the task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1. Load the `winequality-white.csv` dataset from the CSV file and display the first 5 rows, and display a histogram of the quality values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
      "0            7.0              0.27         0.36            20.7      0.045   \n",
      "1            6.3              0.30         0.34             1.6      0.049   \n",
      "2            8.1              0.28         0.40             6.9      0.050   \n",
      "3            7.2              0.23         0.32             8.5      0.058   \n",
      "4            7.2              0.23         0.32             8.5      0.058   \n",
      "\n",
      "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
      "0                 45.0                 170.0   1.0010  3.00       0.45   \n",
      "1                 14.0                 132.0   0.9940  3.30       0.49   \n",
      "2                 30.0                  97.0   0.9951  3.26       0.44   \n",
      "3                 47.0                 186.0   0.9956  3.19       0.40   \n",
      "4                 47.0                 186.0   0.9956  3.19       0.40   \n",
      "\n",
      "   alcohol  quality  \n",
      "0      8.8        6  \n",
      "1      9.5        6  \n",
      "2     10.1        6  \n",
      "3      9.9        6  \n",
      "4      9.9        6  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "data_path = './data/winequality-white.csv'\n",
    "data = pd.read_csv(data_path, sep = ';')\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   0.,    0.,    0.,   20.,  163., 1457., 2198.,  880.,  175.,\n",
       "           5.,    0.]),\n",
       " array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.]),\n",
       " <BarContainer object of 11 artists>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dfHist = plt.hist(df['quality'], bins = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], align = 'left', edgecolor = 'w')\n",
    "plt.xticks([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "dfHist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2. Split the dataset into features `X` and target variables `y`, and then split them into training and test sets using a 70/30 split, using a random seed of 43. Display the shapes of the training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (3428, 11)\n",
      "X_test shape:  (1470, 11)\n",
      "y_train shape: (3428,)\n",
      "y_test shape:  (1470,)\n"
     ]
    }
   ],
   "source": [
    "X = df.loc[:, 'fixed acidity':'alcohol'].values\n",
    "y = df['quality'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size= .3,\n",
    "                                                    random_state = 43,\n",
    "                                                    shuffle = True)\n",
    "print(\"X_train shape: {}\".format(X_train.shape))\n",
    "print(\"X_test shape:  {}\".format(X_test.shape))\n",
    "print(\"y_train shape: {}\".format(y_train.shape))\n",
    "print(\"y_test shape:  {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3. Instantiate the three regression models: `LinearRegression`, `KNeighborsRegressor`, and `DecisionTreeRegressor`.\n",
    "\n",
    "For the `KNeighborsRegressor`, set the number of neighbors to 5 and the distance metric to Euclidian distance. For the `DecisionTreeRegressor`, set the maximum depth to 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_model = LinearRegression()\n",
    "knn_model = KNeighborsRegressor( metric= 'euclidean', n_neighbors = 5)\n",
    "dt_model = DecisionTreeRegressor( max_depth = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Fit each model on the training data and predict the target variable for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_model.fit(X_train, y_train)\n",
    "knn_model.fit(X_train, y_train)\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_lin = lin_model.predict(X_test)\n",
    "y_pred_knn = knn_model.predict(X_test)\n",
    "y_pred_dt = dt_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Implement functions to calculate the Residual Standard Error (RSE) and R-squared for each model.\n",
    "\n",
    "The Residual Sum of Squares (RSS) and Total Sum of Squares (TSS) are defined respectively as $ RSS = \\sum_{i=1}^{n}(y_i - \\hat{y_i})^2 $ and $ TSS = \\sum_{i=1}^{n}(y_i - \\bar{y})^2 $, where $y_i$ is the i-th observed value of the dependent variable, $\\hat{y_i}$ is the predicted value of the dependent variable for the i-th observation, and the sum is taken over all n observations.\n",
    "\n",
    "The Residual Standard Error (RSE) is define as:\n",
    "\n",
    "$$ RSE = \\sqrt{\\frac{\\sum_{i=1}^{n}(y_i - \\hat{y_i})^2}{n - p - 1}} = \\sqrt{\\frac{RSS}{n - p - 1}} $$\n",
    "\n",
    "where n is the number of observations, and p is the number of parameters in the model.\n",
    "\n",
    "The $R^2$ (R-squared) is define as:\n",
    "\n",
    "$$ R^2 = 1 - \\frac{\\sum_{i=1}^{n}(y_i - \\hat{y_i})^2}{\\sum_{i=1}^{n}(y_i - \\bar{y})^2} = 1 - \\frac{RSS}{TSS}$$\n",
    "\n",
    "**Implement the functions `rse()` and `r_squared()` based on the above formulas to use for the calculations.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rse(y_test, y_pred):\n",
    "    rss = np.sum((y_test - y_pred)**2)\n",
    "    rse = np.sqrt(rss/(len(y_test)-10-1))\n",
    "    return rse\n",
    "\n",
    "def r_squared(y_test, y_pred):\n",
    "    y_mean = np.mean(y_test)\n",
    "    rss = np.sum((y_test - y_pred)**2)\n",
    "    tss = np.sum((y_test - y_mean)**2)\n",
    "    rsqrd = 1 - (rss/tss)\n",
    "    return rsqrd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Use your functions to calculate the RSE and R-squared for each model using the predicted and actual target variables on the test data. Display the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression RSE: 0.7603\n",
      "Linear Regression R-SQRD: 0.2824\n",
      "KNeighbors Regressor RSE: 0.8299\n",
      "KNeighbors Regressor R-SQRD: 0.1449\n",
      "Decision Tree Regressor RSE: 0.7636\n",
      "Decision Tree Regressor R-SQRD: 0.2761\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Linear Regression RSE: ' + f\"{rse(y_test, y_pred_lin):.4f}\")\n",
    "print('Linear Regression R-SQRD: ' + f\"{r_squared(y_test, y_pred_lin):.4f}\")\n",
    "print('KNeighbors Regressor RSE: ' + f\"{rse(y_test, y_pred_knn):.4f}\")\n",
    "print('KNeighbors Regressor R-SQRD: ' + f\"{r_squared(y_test, y_pred_knn):.4f}\")\n",
    "print('Decision Tree Regressor RSE: ' + f\"{rse(y_test, y_pred_dt):.4f}\")\n",
    "print('Decision Tree Regressor R-SQRD: ' + f\"{r_squared(y_test, y_pred_dt):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Compare the performance of each model based on the R-squared and RSE scores. Which model performed the best? Which one performed the worst? What might be the reasons for the differences in performance? Write a brief report summarizing your findings and conclusions. Include a discussion of the strengths and weaknesses of each model and how they might be used in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   Linear Regression has the lowest RSE score (.7603) and the highest R-Squared score (.2824). The Linear model performed the best out of the three, it was able to explain about 28% of the vairability of the data and had and average error of about .76. KNeighbor Regressor has the highest RSE score (.8299) and the lowest R-Squared score (.1449). The KNeighbor model performed the worst out of the three, it was only able to explain about 14% of the variability and had an average error of about .83. Decision Tree Regressor is in between the Linear and KNeighbor models in terms of both RSE score (.7637) and R-Squared score(.2761). The Decision Tree model despite not being the best was a very close second to the Linear model. It was able to explain 0.63% less of the variablity than the Linear model but 13% more than the KNeighbor model. It's average error was only .0034 more than the Linear model but .0662 less than the KNeighbor model.\n",
    "   \n",
    "   A possible reason for the differences in performance may be the parameters used when creating the models, such as the metric and number of neighbors used for the KNN model and the max depth of the Decision Tree model. It could also be due to the fact that linear regression assumes a relationship between the independent and dependent variables while KNN and Decision Tree regressions do not.\n",
    "   \n",
    "   The strengths of Linear regression is that it because it assumes a linear relationship is present between the independent and dependent variables it is very useful for data sets that do have a linear relationship. It is also quick to train and can thus make quick predictions on new data. While its assumption of a linear relationship is a strength when considering data that does have this relationship, it is not appropriate for data sets that do not have a linear relationship, and as such has trouble predicting data with more complex relationships.\n",
    "   The strenghts of KNN regression is that it makes no assumption on the relationsihp between independent and dependent variables, and can therefore be useful for evaluating data sets that do not have a linear relationship. But, KNN regression does take a lot of memory and computations to run, thus making it inefficient for larger data sets. It's predictive ability depends on the the number of neighbors it's allowed to compare with, as well as the the distance metric it uses.\n",
    "   The strenghts of Decision Tree regression is that it also makes no assumptions on the relationship between predictors and response variables, and so it is also useful for evaluating data sets without a linear relationship. But it is extremely sensitive to changes in the data set that may drastically change the decision tree.\n",
    "\n",
    "   Overall the best choice of regression model may depend on the problem itself, the available data, and if a clear linear relationship exists between the independent and dependent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 [50 points]: Implement KNN Regression from scratch\n",
    "\n",
    "### 1. Implement the KNN Regression algorithm from scratch. Your implementation should include the following functions:\n",
    "\n",
    "- `euclidean_distance(x1, x2)`: calculates the Euclidean distance between two data points.\n",
    "- `get_neighbors(X_train, y_train, x_test, k)`: finds the k nearest neighbors of a test data point in the training dataset.\n",
    "- `predict(X_train, y_train, x_test, k)`: predicts the output variable for a test data point using KNN Regression with k nearest neighbors.\n",
    "- `evaluate(X_train, y_train, X_test, y_test, k)`: evaluates the performance of the KNN Regression algorithm on a test dataset using the Residual Standard Error (RSE) and R-squared metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(x1, x2):\n",
    "    return np.sqrt(np.sum((x1 - x2)**2))\n",
    "\n",
    "def get_neighbors(X_train, y_train, x_test, k):\n",
    "    dist_to_x_test = []*len(X_train)\n",
    "    k_neighbors = []*k\n",
    "    for i in range(len(X_train)):\n",
    "        dist_to_x_test.append((X_train[i], y_train[i], euclidean_distance(X_train[i], x_test)))\n",
    "    dist_to_x_test.sort(key=lambda i: i[2])\n",
    "    for i in range(k):\n",
    "        k_neighbors.append((dist_to_x_test[i][0], dist_to_x_test[i][1]))\n",
    "    return k_neighbors\n",
    "    \n",
    "def predict(X_train, y_train, x_test, k):\n",
    "    k_neighbors = get_neighbors(X_train, y_train, x_test, k)\n",
    "    y_pred = np.mean([neighbor[1] for neighbor in k_neighbors])\n",
    "    return y_pred\n",
    "    \n",
    "def evaluate(X_train, y_train, X_test, y_test, k):\n",
    "    y_preds = np.array([predict(X_train, y_train, x_test_pnt, k) for x_test_pnt in X_test])\n",
    "    \n",
    "    rss = np.sum((y_test - y_preds)**2)\n",
    "    rse = np.sqrt(rss/(len(y_test)-10-1))\n",
    "\n",
    "    \n",
    "    y_mean = np.mean(y_test)\n",
    "    tss = np.sum((y_test - y_mean)**2)\n",
    "    rsqrd = 1 - (rss/tss)\n",
    "    return rse, rsqrd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Train your KNN Regression model on the training dataset using k=5 and the same train/test splits that you used in Task 1. Evaluate the performance of your KNN Regression model on the testing dataset using the evaluate function. Report the RSE and R-squared values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residual Standard Error: 0.8299\n",
      "R-squared: 0.1449\n"
     ]
    }
   ],
   "source": [
    "data_path = './data/winequality-white.csv'\n",
    "data = pd.read_csv(data_path, sep = ';')\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "X = df.loc[:, 'fixed acidity':'alcohol'].values\n",
    "y = df['quality'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size= .3,\n",
    "                                                    random_state = 43,\n",
    "                                                    shuffle = True)\n",
    "\n",
    "rse, rsqrd = evaluate(X_train, y_train, X_test, y_test, 5)\n",
    "\n",
    "print(\"Residual Standard Error: \" + f\"{rse:.4f}\")\n",
    "print(\"R-squared: \" + f\"{rsqrd:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Write a brief report summarizing your observations. Include a discussion of the strengths and weaknesses of your implementation of KNN Regression. How does your implementation compare to the Scikit-learn implementation? Are there differences in the prediction accuracy and speed of the two implementations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The strengths of my implementation KNN Regression from scratch is that it's code is clear and concise, but more notable is it's accuracy. My implementation of KNN Regression appears to be as accurate as the Scikit-learn implementation. Above I only displayed four decimal places for the  Residual Standard Error and R-squared calculations, for both the Scikit-learn version and my version of KNN Regression, for simplicity. But, I also checked to see how they compare to each other for up to 20 decimal places, both have a RSE of 0.82990705277398546524 and an R-squared score of 0.14485526190867659135. \n",
    "The blatant weakness of my implementation of KNN Regression is that it is inefficient comapred to the Scikit-learn implementation. While it does take a few seconds to independently initiate the execution of each cell for the Scikit-learn version, each cell clearly takes less than a second to execute. But when timed my implementation took roughly a minute and a half to fully execute."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "aad5b032669a22d10aade9b71c16d8cc65c4a37ed7e83af7b59e17b6dee418eb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
